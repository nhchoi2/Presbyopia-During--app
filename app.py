import streamlit as st
from keras.models import load_model
from PIL import Image, ImageOps
import numpy as np
from config import API_KEY, DEBUG_MODE
import openai

# OpenAI API í‚¤ ì„¤ì •
if not API_KEY:
    raise ValueError("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
openai.api_key = API_KEY

# ë””ë²„ê·¸ ëª¨ë“œ ì²´í¬
if DEBUG_MODE:
    st.info("ğŸ”§ ë””ë²„ê·¸ ëª¨ë“œ í™œì„±í™” ì¤‘ì…ë‹ˆë‹¤.")

# AI ëª¨ë¸ ë¡œë“œ
MODEL_PATH = "model/keras_model.h5"
LABELS_PATH = "model/labels.txt"

@st.cache_resource
def load_ai_model():
    return load_model(MODEL_PATH, compile=False)

@st.cache_resource
def load_labels():
    with open(LABELS_PATH, "r", encoding="utf-8") as f:
        return [line.strip() for line in f.readlines()]

model = load_ai_model()
labels = load_labels()

# Streamlit UI
st.title("ğŸ” ë™ì•ˆ vs ë…¸ì•ˆ íŒë³„ê¸°")
st.info("ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ë©´ AIê°€ ë¶„ì„í•˜ì—¬ ë™ì•ˆì¸ì§€ ë…¸ì•ˆì¸ì§€ íŒë³„í•´ë“œë¦½ë‹ˆë‹¤!")

# íŒŒì¼ ì—…ë¡œë“œ
uploaded_file = st.file_uploader("ì‚¬ì§„ì„ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["jpg", "png", "jpeg"])

if uploaded_file:
    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ í‘œì‹œ
    image = Image.open(uploaded_file)
    st.image(image, caption="ì—…ë¡œë“œí•œ ì´ë¯¸ì§€", use_column_width=True)

    # ì´ë¯¸ì§€ ì „ì²˜ë¦¬
    size = (224, 224)
    image = ImageOps.fit(image, size, Image.Resampling.LANCZOS)
    image_array = np.asarray(image) / 127.5 - 1
    img = np.expand_dims(image_array, axis=0)

    # AI ëª¨ë¸ë¡œ ì˜ˆì¸¡
    prediction = model.predict(img)
    result_idx = np.argmax(prediction)
    result_label = labels[result_idx]
    confidence_score = prediction[0][result_idx]

    # ê²°ê³¼ ì¶œë ¥
    st.subheader("ğŸ“Œ AI íŒë³„ ê²°ê³¼")
    st.info(f"**ì´ ì–¼êµ´ì€ {result_label}ì…ë‹ˆë‹¤.** í™•ë¥ : {confidence_score:.2%}")

    # ê²°ê³¼ì— ë”°ë¥¸ í”¼ë“œë°±
    feedback = {
        "ë™ì•ˆ": [
            "ì™„ë²½í•œ ë™ì•ˆ! ì‹ ë¶„ì¦ ê²€ì‚¬ ê±±ì • ì—†ê² ë„¤ìš”!",
            "ì–´ë¦´ ë•Œë¶€í„° ì§€ê¸ˆê¹Œì§€ ë˜‘ê°™ì€ ì–¼êµ´?!",
            "ë™ì•ˆ ìœ ì§€ ë¹„ë²• ì¢€ ì•Œë ¤ì£¼ì„¸ìš”!",
            "ì´ˆë“±í•™ìƒ ë•Œë„ ì´ ì–¼êµ´ì´ì—ˆì„ ë“¯?",
            "ì–´ë””ì„œ ì‹œê°„ì„ ë©ˆì¶”ì…¨ë‚˜ìš”?"
        ],
        "ë…¸ì•ˆ": [
            "ë…¸ì•ˆì´ì§€ë§Œ ë©‹ìˆì–´ìš”! ì‹ ë¢°ê° í­ë°œ!",
            "ì„¸ì›”ì˜ í”ì ì´ ëŠê»´ì§€ëŠ” ì–¼êµ´â€¦ í•˜ì§€ë§Œ ì¹´ë¦¬ìŠ¤ë§ˆëŠ” ìµœê³ !",
            "ì¸ìƒì˜ ê¹Šì´ê°€ ë³´ì´ëŠ” ì–¼êµ´!",
            "ë©‹ì§„ ì¤‘í›„í•œ ë§¤ë ¥ì´ ëŠê»´ì ¸ìš”!",
            "ë…¸ì•ˆì´ë¼ê³ ìš”? ê·¸ëƒ¥ ì–´ë¥¸ìŠ¤ëŸ¬ìš´ ê±°ì£ !"
        ]
    }

    random_feedback = np.random.choice(feedback[result_label])
    st.write(f"ğŸ’¬ **AI í”¼ë“œë°±:** {random_feedback}")

    # AI ì±—ë´‡ ì§ˆë¬¸
    st.subheader("ğŸ¤– ë™ì•ˆ/ë…¸ì•ˆ ê´€ë ¨ AI ìƒë‹´")
    question = st.text_input("ë™ì•ˆ/ë…¸ì•ˆ ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")
    if question:
        with st.spinner("AIê°€ ë‹µë³€ì„ ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": question}]
                )
                answer = response["choices"][0]["message"]["content"]
                st.write(f"ğŸ¤– AI ë‹µë³€: {answer}")
            except Exception as e:
                st.error(f"âŒ OpenAI API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

